---
title: "test"
author: "Amie Thomas"
date: "2023-11-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(tidyverse)
```


#load in dataset 
```{r setup, include=FALSE}
art <- read_dta("nanda_artsentrec_tract_2003-2017_01P.dta") #art/entertainment
```

#use only 2017 because it has the most up to date information for analysis and also to make computation easier. Getting rid of uninformative columns
```{r}
art_17 <- art|>
  filter(year == "2017")|>
  select(tract_fips10, year, population, popden_7111, popden_7112, popden_712,popden_51912, 
        popden_7131, popden_7132, popden_7139, popden_71394)
```

#check for 0's and handle
```{r}
missing_values <- any(is.na(art_17))
rows_with_missing <- art_17[!complete.cases(art_17), ]
#some census tracts have a 0 population. Can't do anything with this information so best to remove
#are these true 0's? Possible limitations in the data 

art_17_filt <- art_17 |>
  filter(population != 0) #no need for population of 0

miss_again <- any(is.na(art_17_filt)) #no missing values now

```

```{r}
#Northeast (Region 1):
# Connecticut: State Code - 09
# Maine: State Code - 23
# Massachusetts: State Code - 25
# New Hampshire: State Code - 33
# Rhode Island: State Code - 44
# Vermont: State Code - 50
# New Jersey: State Code - 34
# New York: State Code - 36
# Pennsylvania: State Code - 42


#Midwest (Region 2):
# Illinois: State Code - 17
# Indiana: State Code - 18
# Michigan: State Code - 26
# Ohio: State Code - 39
# Wisconsin: State Code - 55
# Iowa: State Code - 19
# Kansas: State Code - 20
# Minnesota: State Code - 27
# Missouri: State Code - 29
# Nebraska: State Code - 31
# North Dakota: State Code - 38
# South Dakota: State Code - 46


#South (Region 3):
# Delaware: State Code - 10
# Florida: State Code - 12
# Georgia: State Code - 13
# Maryland: State Code - 24
# North Carolina: State Code - 37
# South Carolina: State Code - 45
# Virginia: State Code - 51
# West Virginia: State Code - 54
# Alabama: State Code - 01
# Kentucky: State Code - 21
# Mississippi: State Code - 28
# Tennessee: State Code - 47
# Arkansas: State Code - 05
# Louisiana: State Code - 22
# Oklahoma: State Code - 40
# Texas: State Code - 48

#West (Region 4)
# Arizona: State Code - 04
# Colorado: State Code - 08
# Idaho: State Code - 16
# Montana: State Code - 30
# Nevada: State Code - 32
# New Mexico: State Code - 35
# Utah: State Code - 49
# Wyoming: State Code - 56
# Alaska: State Code - 02
# California: State Code - 06
# Hawaii: State Code - 15
# Oregon: State Code - 41
# Washington: State Code - 53
```

```{r}
#match the state census tracts to census regions to easier sampling

get_census_region <- function(code) {
  census_region_map <- c(
    "10" = "South", "12" = "South", "13" = "South", "24" = "South", 
    "37" = "South", "45" = "South", "51" = "South", "54" = "South", 
    "01" = "South", "21" = "South", "28" = "South", "47" = "South", 
    "05" = "South", "22" = "South", "40" = "South", "48" = "South", 
    "09" = "Northeast", "23" = "Northeast", "25" = "Northeast", 
    "33" = "Northeast", "44" = "Northeast", "50" = "Northeast", 
    "34" = "Northeast", "36" = "Northeast", "42" = "Northeast", 
    "17" = "Midwest", "18" = "Midwest", "26" = "Midwest", "39" = "Midwest", 
    "55" = "Midwest", "19" = "Midwest", "20" = "Midwest", "27" = "Midwest", 
    "29" = "Midwest", "31" = "Midwest", "38" = "Midwest", "46" = "Midwest", 
    "04" = "West", "08" = "West", "16" = "West", "30" = "West", 
    "32" = "West", "35" = "West", "49" = "West", "56" = "West", 
    "02" = "West", "06" = "West", "15" = "West", "41" = "West", "53" = "West"
  )
  
  region <- census_region_map[substr(code, 1, 2)]
  if (is.na(region)) {
    region <- "Unknown"  
  }
  
  return(region)
}

art_17_filt <- art_17_filt |>
  mutate(census_region = sapply(tract_fips10, get_census_region))

art_17_filt <- art_17_filt |>
  relocate(census_region, .after = tract_fips10)

unknown_regions <- art_17_filt |>
  filter(census_region == "Unknown")

#all of the tract codes are matched to their respective regions. There are 179 unknowns which don't make sense since all of them should have a match. Upon further inspection, the issue is the code begininng with 11. There is no matching state for this. It turns out that this code is for Washington DC which is a federal district and not a state. 

#we will change these unknowns to correspond with "south" to reflect the actual census region that it is a part of 

# Update census_region for census tracts starting with "11" to be "South"
art_17_filt$census_region[startsWith(art_17_filt$tract_fips10, "11")] <- "South"

```

#load in dataset 
```{r}
vice <- read_dta("nanda_lqtbcon_tract_2003-2017_01P.dta") #liquor stores/tobacco 
```

#use only 2017 because it has the most up to date information for analysis and also to make computation easier. Getting rid of uninformative columns
```{r}
vice_17 <- vice|>
  filter(year == "2017")|>
  select(tract_fips10, year, population, popden_4453, popden_453991)
```

#check for 0's and handle
```{r}
missing_values_2 <- any(is.na(vice_17))
rows_with_missing_2 <- vice_17[!complete.cases(vice_17), ]
#some census tracts have a 0 population. Can't do anything with this information so best to remove
#are these true 0's? Possible limitations in the data 

vice_17_filt <- vice_17 |>
  filter(population != 0) #no need for population of 0

miss_again_2 <- any(is.na(vice_17_filt)) #no missing values now

```


```{r}
get_census_region_2 <- function(code_2) {
  census_region_map_2 <- c(
    "10" = "South", "12" = "South", "13" = "South", "24" = "South", 
    "37" = "South", "45" = "South", "51" = "South", "54" = "South", 
    "01" = "South", "21" = "South", "28" = "South", "47" = "South", 
    "05" = "South", "22" = "South", "40" = "South", "48" = "South", 
    "09" = "Northeast", "23" = "Northeast", "25" = "Northeast", 
    "33" = "Northeast", "44" = "Northeast", "50" = "Northeast", 
    "34" = "Northeast", "36" = "Northeast", "42" = "Northeast", 
    "17" = "Midwest", "18" = "Midwest", "26" = "Midwest", "39" = "Midwest", 
    "55" = "Midwest", "19" = "Midwest", "20" = "Midwest", "27" = "Midwest", 
    "29" = "Midwest", "31" = "Midwest", "38" = "Midwest", "46" = "Midwest", 
    "04" = "West", "08" = "West", "16" = "West", "30" = "West", 
    "32" = "West", "35" = "West", "49" = "West", "56" = "West", 
    "02" = "West", "06" = "West", "15" = "West", "41" = "West", "53" = "West"
  )
  
  region_2 <- census_region_map_2[substr(code_2, 1, 2)]
  if (is.na(region_2)) {
    region_2 <- "Unknown"  
  }
  
  return(region_2)
}

vice_17_filt <- vice_17_filt |>
  mutate(census_region = sapply(tract_fips10, get_census_region_2))

vice_17_filt <- vice_17_filt |>
  relocate(census_region, .after = tract_fips10)

unknown_regions_2 <- vice_17_filt |>
  filter(census_region == "Unknown")

vice_17_filt$census_region[startsWith(vice_17_filt$tract_fips10, "11")] <- "South"

```

#load in dataset 
```{r}
social_orgs <- read_dta("nanda_relcivsoc_tract_2003-2017_01P.dta") #social organization
```

```{r}
social_orgs_17 <- social_orgs|>
  filter(year == "2017")|>
  select(tract_fips10, year, population, popden_8131, popden_8134)
```

#check for 0's and handle
```{r}
missing_values_3 <- any(is.na(social_orgs_17))
rows_with_missing_3 <- social_orgs_17[!complete.cases(social_orgs_17), ]

social_orgs_17_filt <- social_orgs_17 |>
  filter(population != 0) #no need for population of 0

miss_again_3 <- any(is.na(social_orgs_17_filt)) #no missing values now

```


```{r}
get_census_region_3 <- function(code_3) {
  census_region_map_3 <- c(
    "10" = "South", "12" = "South", "13" = "South", "24" = "South", 
    "37" = "South", "45" = "South", "51" = "South", "54" = "South", 
    "01" = "South", "21" = "South", "28" = "South", "47" = "South", 
    "05" = "South", "22" = "South", "40" = "South", "48" = "South", 
    "09" = "Northeast", "23" = "Northeast", "25" = "Northeast", 
    "33" = "Northeast", "44" = "Northeast", "50" = "Northeast", 
    "34" = "Northeast", "36" = "Northeast", "42" = "Northeast", 
    "17" = "Midwest", "18" = "Midwest", "26" = "Midwest", "39" = "Midwest", 
    "55" = "Midwest", "19" = "Midwest", "20" = "Midwest", "27" = "Midwest", 
    "29" = "Midwest", "31" = "Midwest", "38" = "Midwest", "46" = "Midwest", 
    "04" = "West", "08" = "West", "16" = "West", "30" = "West", 
    "32" = "West", "35" = "West", "49" = "West", "56" = "West", 
    "02" = "West", "06" = "West", "15" = "West", "41" = "West", "53" = "West"
  )
  
  region_3 <- census_region_map_3[substr(code_3, 1, 2)]
  if (is.na(region_3)) {
    region_3 <- "Unknown"  
  }
  
  return(region_3)
}

social_orgs_17_filt <- social_orgs_17_filt |>
  mutate(census_region = sapply(tract_fips10, get_census_region_3))

social_orgs_17_filt <- social_orgs_17_filt |>
  relocate(census_region, .after = tract_fips10)

unknown_regions_3 <- social_orgs_17_filt |>
  filter(census_region == "Unknown")

social_orgs_17_filt$census_region[startsWith(social_orgs_17_filt$tract_fips10, "11")] <- "South"

```

#load in dataset 
```{r}
ses <- read_dta("nanda_ses_tract_2008-2017_04P.dta") #ses
```

```{r}
ses_select <- ses|>
  select(tract_fips10, totpop13_17, ped1_13_17, ped2_13_17, ped3_13_17)
```

#check for 0's and handle
```{r}
missing_values_4 <- any(is.na(ses_select))
rows_with_missing_4 <- ses_select[!complete.cases(ses_select), ]

ses_select_filt <- ses_select |>
  filter(!is.na(totpop13_17) & totpop13_17 != 0)

miss_again_4 <- any(is.na(ses_select_filt)) #check for missing values

rows_with_missing_5 <- ses_select_filt[!complete.cases(ses_select_filt), ]

#some rows have no education information. We don't need these. 

cleaned_ses <- na.omit(ses_select_filt)

miss_again_5 <- any(is.na(cleaned_ses))
```


```{r}

get_census_region_4 <- function(code_4) {
  census_region_map_4 <- c(
    "10" = "South", "12" = "South", "13" = "South", "24" = "South", 
    "37" = "South", "45" = "South", "51" = "South", "54" = "South", 
    "01" = "South", "21" = "South", "28" = "South", "47" = "South", 
    "05" = "South", "22" = "South", "40" = "South", "48" = "South", 
    "09" = "Northeast", "23" = "Northeast", "25" = "Northeast", 
    "33" = "Northeast", "44" = "Northeast", "50" = "Northeast", 
    "34" = "Northeast", "36" = "Northeast", "42" = "Northeast", 
    "17" = "Midwest", "18" = "Midwest", "26" = "Midwest", "39" = "Midwest", 
    "55" = "Midwest", "19" = "Midwest", "20" = "Midwest", "27" = "Midwest", 
    "29" = "Midwest", "31" = "Midwest", "38" = "Midwest", "46" = "Midwest", 
    "04" = "West", "08" = "West", "16" = "West", "30" = "West", 
    "32" = "West", "35" = "West", "49" = "West", "56" = "West", 
    "02" = "West", "06" = "West", "15" = "West", "41" = "West", "53" = "West"
  )
  
  region_4 <- census_region_map_4[substr(code_4, 1, 2)]
  if (is.na(region_4)) {
    region_4 <- "Unknown"  
  }
  
  return(region_4)
}

cleaned_ses <- cleaned_ses |>
  mutate(census_region = sapply(tract_fips10, get_census_region_4))

cleaned_ses <- cleaned_ses |>
  relocate(census_region, .after = tract_fips10)

unknown_regions_4 <- cleaned_ses |>
  filter(census_region == "Unknown")

cleaned_ses$census_region[startsWith(cleaned_ses$tract_fips10, "11")] <- "South"

```

#combine all datasets
```{r}

community_data <- left_join(art_17_filt, vice_17_filt, by = "tract_fips10") |>
               left_join(social_orgs_17_filt, by = "tract_fips10") |>
               left_join(cleaned_ses, by = "tract_fips10")

#check for NA's

test <- any(is.na(community_data))

rows_with_missing_6 <- community_data[!complete.cases(community_data), ]

#for some reason, even after cleaning it of na's they pop back in. Since we already know that this is because 8 of the census tracts are missing education information we will not be using them. we will get rid of them again. 

community_data <- na.omit(community_data)

test_2 <- any(is.na(community_data))

#get rid of duplicate columns

community_data <- community_data |>
  select(-census_region.y, -year.y, -population.y, -census_region.x.x, -census_region.y.y, -population, -year, -totpop13_17)

community_data <- community_data |>
  rename(census_region = census_region.x, year = year.x, population = population.x)

 community_data <- community_data |>
   rename(
     performing_arts = popden_7111,   
     spectator_sports_orgs = popden_7112,
     museums = popden_712,  
     libraries = popden_51912,  
     amusement_parks = popden_7131,  
     casinos = popden_7132,
     recreation = popden_7139,  
     fitness = popden_71394,  
     liquor_store = popden_4453,  
     tobacco = popden_453991,
     religious_orgs = popden_8131,   
     social_orgs = popden_8134,  
     less_than_hs = ped1_13_17,    
     hs_some_college = ped2_13_17,    
     bach_and_higher = ped3_13_17, 
)

```

#sample proportionately might need to sample at all
```{r}
# proportions <- table(community_data$census_region) / nrow(community_data) # Calculate proportions
# 
# downsize_proportions <- round(proportions * 10000) # Target 10000 total observations
# 
# downsample_groups <- function(community_data, target_counts) {
#   sampled_data <- data.frame()  # Create an empty data frame to store the sampled data
#   
#   for (i in 1:length(target_counts)) {  # Loop through each group's target count
#     group_subset <- subset(community_data, census_region == names(target_counts)[i])  # Subset the data for a specific group
#     sampled_indices <- sample(1:nrow(group_subset), target_counts[i])  # Randomly sample indices based on the target count
#     sampled_group <- group_subset[sampled_indices, ]  # Store the sampled group data
#     
#     sampled_data <- rbind(sampled_data, sampled_group)  # Append sampled rows to the sampled_data data frame
#   }
#   
#   return(sampled_data)  # Return the final sampled dataset
# }

# sampled_community_data <- downsample_groups(community_data, downsize_proportions)
```


### EDA Begins ###

```{r}
str(community_data)
summary(community_data)
cor(community_data)
```



