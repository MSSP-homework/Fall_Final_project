---
title: "test"
author: "Amie Thomas"
date: "2023-11-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("caret")
#install.packages("recipes")
#install.packages("lme4")
#install.packages("ROSE")
library(haven)
library(tidyverse)
library(ggplot2)
library(knitr)
library(usmap)
library(caret)
library(lme4)
library(ROSE)
```



#load in dataset 
```{r setup, include=FALSE}
art <- read_dta("nanda_artsentrec_tract_2003-2017_01P.dta") #art/entertainment
```

#use only 2017 because it has the most up to date information for analysis and also to make computation easier. Getting rid of uninformative columns
```{r}
art_17 <- art|>
  filter(year == "2017")|>
  dplyr::select(tract_fips10, year, population, popden_7111, popden_7112, popden_712, popden_51912, 
        popden_7131, popden_7132, popden_7139, popden_71394)
```

#check for 0's and handle
```{r}
missing_values <- any(is.na(art_17))
rows_with_missing <- art_17[!complete.cases(art_17), ]
#some census tracts have a 0 population. Can't do anything with this information so best to remove
#are these true 0's? Possible limitations in the data 

art_17_filt <- art_17 |>
  filter(population != 0) #no need for population of 0

miss_again <- any(is.na(art_17_filt)) #no missing values now

```

```{r}
#Northeast (Region 1):
# Connecticut: State Code - 09
# Maine: State Code - 23
# Massachusetts: State Code - 25
# New Hampshire: State Code - 33
# Rhode Island: State Code - 44
# Vermont: State Code - 50
# New Jersey: State Code - 34
# New York: State Code - 36
# Pennsylvania: State Code - 42


#Midwest (Region 2):
# Illinois: State Code - 17
# Indiana: State Code - 18
# Michigan: State Code - 26
# Ohio: State Code - 39
# Wisconsin: State Code - 55
# Iowa: State Code - 19
# Kansas: State Code - 20
# Minnesota: State Code - 27
# Missouri: State Code - 29
# Nebraska: State Code - 31
# North Dakota: State Code - 38
# South Dakota: State Code - 46


#South (Region 3):
# Delaware: State Code - 10
# Florida: State Code - 12
# Georgia: State Code - 13
# Maryland: State Code - 24
# North Carolina: State Code - 37
# South Carolina: State Code - 45
# Virginia: State Code - 51
# West Virginia: State Code - 54
# Alabama: State Code - 01
# Kentucky: State Code - 21
# Mississippi: State Code - 28
# Tennessee: State Code - 47
# Arkansas: State Code - 05
# Louisiana: State Code - 22
# Oklahoma: State Code - 40
# Texas: State Code - 48

#West (Region 4)
# Arizona: State Code - 04
# Colorado: State Code - 08
# Idaho: State Code - 16
# Montana: State Code - 30
# Nevada: State Code - 32
# New Mexico: State Code - 35
# Utah: State Code - 49
# Wyoming: State Code - 56
# Alaska: State Code - 02
# California: State Code - 06
# Hawaii: State Code - 15
# Oregon: State Code - 41
# Washington: State Code - 53
```

```{r}
#match the state census tracts to census regions to easier sampling

get_census_region <- function(code) {
  census_region_map <- c(
    "10" = "South", "12" = "South", "13" = "South", "24" = "South", 
    "37" = "South", "45" = "South", "51" = "South", "54" = "South", 
    "01" = "South", "21" = "South", "28" = "South", "47" = "South", 
    "05" = "South", "22" = "South", "40" = "South", "48" = "South", 
    "09" = "Northeast", "23" = "Northeast", "25" = "Northeast", 
    "33" = "Northeast", "44" = "Northeast", "50" = "Northeast", 
    "34" = "Northeast", "36" = "Northeast", "42" = "Northeast", 
    "17" = "Midwest", "18" = "Midwest", "26" = "Midwest", "39" = "Midwest", 
    "55" = "Midwest", "19" = "Midwest", "20" = "Midwest", "27" = "Midwest", 
    "29" = "Midwest", "31" = "Midwest", "38" = "Midwest", "46" = "Midwest", 
    "04" = "West", "08" = "West", "16" = "West", "30" = "West", 
    "32" = "West", "35" = "West", "49" = "West", "56" = "West", 
    "02" = "West", "06" = "West", "15" = "West", "41" = "West", "53" = "West"
  )
  
  region <- census_region_map[substr(code, 1, 2)]
  if (is.na(region)) {
    region <- "Unknown"  
  }
  
  return(region)
}

art_17_filt <- art_17_filt |>
  mutate(census_region = sapply(tract_fips10, get_census_region))

art_17_filt <- art_17_filt |>
  relocate(census_region, .after = tract_fips10)

unknown_regions <- art_17_filt |>
  filter(census_region == "Unknown")

#all of the tract codes are matched to their respective regions. There are 179 unknowns which don't make sense since all of them should have a match. Upon further inspection, the issue is the code beginning with 11. There is no matching state for this. It turns out that this code is for Washington DC which is a federal district and not a state. 

#we will change these unknowns to correspond with "south" to reflect the actual census region that it is a part of 

# Update census_region for census tracts starting with "11" to be "South"
art_17_filt$census_region[startsWith(art_17_filt$tract_fips10, "11")] <- "South"

```

#load in dataset 
```{r}
vice <- read_dta("nanda_lqtbcon_tract_2003-2017_01P.dta") #liquor stores/tobacco 
```

#use only 2017 because it has the most up to date information for analysis and also to make computation easier. Getting rid of uninformative columns
```{r}
vice_17 <- vice|>
  filter(year == "2017")|>
  dplyr::select(tract_fips10, year, population, popden_4453, popden_453991)
```

#check for 0's and handle
```{r}
missing_values_2 <- any(is.na(vice_17))
rows_with_missing_2 <- vice_17[!complete.cases(vice_17), ]
#some census tracts have a 0 population. Can't do anything with this information so best to remove
#are these true 0's? Possible limitations in the data 

vice_17_filt <- vice_17 |>
  filter(population != 0) #no need for population of 0

miss_again_2 <- any(is.na(vice_17_filt)) #no missing values now

```


```{r}
get_census_region_2 <- function(code_2) {
  census_region_map_2 <- c(
    "10" = "South", "12" = "South", "13" = "South", "24" = "South", 
    "37" = "South", "45" = "South", "51" = "South", "54" = "South", 
    "01" = "South", "21" = "South", "28" = "South", "47" = "South", 
    "05" = "South", "22" = "South", "40" = "South", "48" = "South", 
    "09" = "Northeast", "23" = "Northeast", "25" = "Northeast", 
    "33" = "Northeast", "44" = "Northeast", "50" = "Northeast", 
    "34" = "Northeast", "36" = "Northeast", "42" = "Northeast", 
    "17" = "Midwest", "18" = "Midwest", "26" = "Midwest", "39" = "Midwest", 
    "55" = "Midwest", "19" = "Midwest", "20" = "Midwest", "27" = "Midwest", 
    "29" = "Midwest", "31" = "Midwest", "38" = "Midwest", "46" = "Midwest", 
    "04" = "West", "08" = "West", "16" = "West", "30" = "West", 
    "32" = "West", "35" = "West", "49" = "West", "56" = "West", 
    "02" = "West", "06" = "West", "15" = "West", "41" = "West", "53" = "West"
  )
  
  region_2 <- census_region_map_2[substr(code_2, 1, 2)]
  if (is.na(region_2)) {
    region_2 <- "Unknown"  
  }
  
  return(region_2)
}

vice_17_filt <- vice_17_filt |>
  mutate(census_region = sapply(tract_fips10, get_census_region_2))

vice_17_filt <- vice_17_filt |>
  relocate(census_region, .after = tract_fips10)

unknown_regions_2 <- vice_17_filt |>
  filter(census_region == "Unknown")

vice_17_filt$census_region[startsWith(vice_17_filt$tract_fips10, "11")] <- "South"

```

#load in dataset 
```{r}
social_orgs <- read_dta("nanda_relcivsoc_tract_2003-2017_01P.dta") #social organization
```

```{r}
social_orgs_17 <- social_orgs|>
  filter(year == "2017")|>
  dplyr::select(tract_fips10, year, population, popden_8131, popden_8134)
```

#check for 0's and handle
```{r}
missing_values_3 <- any(is.na(social_orgs_17))
rows_with_missing_3 <- social_orgs_17[!complete.cases(social_orgs_17), ]

social_orgs_17_filt <- social_orgs_17 |>
  filter(population != 0) #no need for population of 0

miss_again_3 <- any(is.na(social_orgs_17_filt)) #no missing values now

```


```{r}
get_census_region_3 <- function(code_3) {
  census_region_map_3 <- c(
    "10" = "South", "12" = "South", "13" = "South", "24" = "South", 
    "37" = "South", "45" = "South", "51" = "South", "54" = "South", 
    "01" = "South", "21" = "South", "28" = "South", "47" = "South", 
    "05" = "South", "22" = "South", "40" = "South", "48" = "South", 
    "09" = "Northeast", "23" = "Northeast", "25" = "Northeast", 
    "33" = "Northeast", "44" = "Northeast", "50" = "Northeast", 
    "34" = "Northeast", "36" = "Northeast", "42" = "Northeast", 
    "17" = "Midwest", "18" = "Midwest", "26" = "Midwest", "39" = "Midwest", 
    "55" = "Midwest", "19" = "Midwest", "20" = "Midwest", "27" = "Midwest", 
    "29" = "Midwest", "31" = "Midwest", "38" = "Midwest", "46" = "Midwest", 
    "04" = "West", "08" = "West", "16" = "West", "30" = "West", 
    "32" = "West", "35" = "West", "49" = "West", "56" = "West", 
    "02" = "West", "06" = "West", "15" = "West", "41" = "West", "53" = "West"
  )
  
  region_3 <- census_region_map_3[substr(code_3, 1, 2)]
  if (is.na(region_3)) {
    region_3 <- "Unknown"  
  }
  
  return(region_3)
}

social_orgs_17_filt <- social_orgs_17_filt |>
  mutate(census_region = sapply(tract_fips10, get_census_region_3))

social_orgs_17_filt <- social_orgs_17_filt |>
  relocate(census_region, .after = tract_fips10)

unknown_regions_3 <- social_orgs_17_filt |>
  filter(census_region == "Unknown")

social_orgs_17_filt$census_region[startsWith(social_orgs_17_filt$tract_fips10, "11")] <- "South"

```

#load in dataset 
```{r}
ses <- read_dta("nanda_ses_tract_2008-2017_04P.dta") #ses
```

```{r}
ses_select <- ses|>
  dplyr::select(tract_fips10, totpop13_17, ped1_13_17, ped2_13_17, ped3_13_17)
```

#check for 0's and handle
```{r}
missing_values_4 <- any(is.na(ses_select))
rows_with_missing_4 <- ses_select[!complete.cases(ses_select), ]

ses_select_filt <- ses_select |>
  filter(!is.na(totpop13_17) & totpop13_17 != 0)

miss_again_4 <- any(is.na(ses_select_filt)) #check for missing values

rows_with_missing_5 <- ses_select_filt[!complete.cases(ses_select_filt), ]

#some rows have no education information. We don't need these. 

cleaned_ses <- na.omit(ses_select_filt)

miss_again_5 <- any(is.na(cleaned_ses))
```

#make region column
```{r}

get_census_region_4 <- function(code_4) {
  census_region_map_4 <- c(
    "10" = "South", "12" = "South", "13" = "South", "24" = "South", 
    "37" = "South", "45" = "South", "51" = "South", "54" = "South", 
    "01" = "South", "21" = "South", "28" = "South", "47" = "South", 
    "05" = "South", "22" = "South", "40" = "South", "48" = "South", 
    "09" = "Northeast", "23" = "Northeast", "25" = "Northeast", 
    "33" = "Northeast", "44" = "Northeast", "50" = "Northeast", 
    "34" = "Northeast", "36" = "Northeast", "42" = "Northeast", 
    "17" = "Midwest", "18" = "Midwest", "26" = "Midwest", "39" = "Midwest", 
    "55" = "Midwest", "19" = "Midwest", "20" = "Midwest", "27" = "Midwest", 
    "29" = "Midwest", "31" = "Midwest", "38" = "Midwest", "46" = "Midwest", 
    "04" = "West", "08" = "West", "16" = "West", "30" = "West", 
    "32" = "West", "35" = "West", "49" = "West", "56" = "West", 
    "02" = "West", "06" = "West", "15" = "West", "41" = "West", "53" = "West"
  )
  
  region_4 <- census_region_map_4[substr(code_4, 1, 2)]
  if (is.na(region_4)) {
    region_4 <- "Unknown"  
  }
  
  return(region_4)
}

cleaned_ses <- cleaned_ses |>
  mutate(census_region = sapply(tract_fips10, get_census_region_4))

cleaned_ses <- cleaned_ses |>
  relocate(census_region, .after = tract_fips10)

unknown_regions_4 <- cleaned_ses |>
  filter(census_region == "Unknown")

cleaned_ses$census_region[startsWith(cleaned_ses$tract_fips10, "11")] <- "South"

```

#combine all datasets
```{r}

community_data <- left_join(art_17_filt, vice_17_filt, by = "tract_fips10") |>
               left_join(social_orgs_17_filt, by = "tract_fips10") |>
               left_join(cleaned_ses, by = "tract_fips10")

#check for NA's

test <- any(is.na(community_data))

rows_with_missing_6 <- community_data[!complete.cases(community_data), ]

#for some reason, even after cleaning it of na's they pop back in. Since we already know that this is because 8 of the census tracts are missing education information we will not be using them. we will get rid of them again. 

community_data <- na.omit(community_data)

test_2 <- any(is.na(community_data))

#get rid of duplicate columns

community_data <- community_data |> 
  dplyr::select(-census_region.y, -year.y, -population.y, -census_region.x.x, -census_region.y.y, -population, -year, -totpop13_17)

community_data <- community_data |>
  rename(census_region = census_region.x, year = year.x, population = population.x)

 community_data <- community_data |> #rename for understanding
   rename(
     performing_arts = popden_7111,   
     spectator_sports_orgs = popden_7112,
     museums = popden_712,  
     libraries = popden_51912,  
     amusement_parks = popden_7131,  
     casinos = popden_7132,
     recreation = popden_7139,  
     fitness = popden_71394,  
     liquor_store = popden_4453,  
     tobacco = popden_453991,
     religious_orgs = popden_8131,   
     social_orgs = popden_8134,  
     less_than_hs = ped1_13_17,    
     hs_some_college = ped2_13_17,    
     bach_and_higher = ped3_13_17, 
)

```

#make states column
```{r}
get_state <- function(code_a) {
  census_state_map <- c(
    "10" = "Delaware", "12" = "Florida", "13" = "Georgia", "24" = "Maryland", 
    "37" = "North Carolina", "45" = "South Carolina", "51" = "Virginia", "54" = "West Virginia", 
    "01" = "Alabama", "21" = "Kentucky", "28" = "Mississippi", "47" = "Tennessee", 
    "05" = "Arkansas", "22" = "Louisiana", "40" = "Oklahoma", "48" = "Texas", 
    "09" = "Connecticut", "23" = "Maine", "25" = "Massachusetts", 
    "33" = "New Hampshire", "44" = "Rhode Island", "50" = "Vermont", 
    "34" = "New Jersey", "36" = "New York", "42" = "Pennsylvania", 
    "17" = "Illinois", "18" = "Indiana", "26" = "Michigan", "39" = "Ohio", 
    "55" = "Wisconsin", "19" = "Iowa", "20" = "Kansas", "27" = "Minnesota", 
    "29" = "Missouri", "31" = "Nebraska", "38" = "North Dakota", "46" = "South Dakota", 
    "04" = "Arizona", "08" = "Colorado", "16" = "Idaho", "30" = "Montana", 
    "32" = "Nevada", "35" = "New Mexico", "49" = "Utah", "56" = "Wyoming", 
    "02" = "Alaska", "06" = "California", "15" = "Hawaii", "41" = "Oregon", "53" = "Washington", "11" = "DC" #district of columbia
  )

state <- census_state_map[substr(code_a, 1, 2)]
  if (is.na(state)) {
    state <- "Unknown"  
  }
  
  return(state)
}

community_data <- community_data |>
  mutate(state = sapply(tract_fips10, get_state))|>
  relocate(state, .before = year )

```

#collaspe edu levels to no college and college education
```{r}
community_data$no_CD <- community_data$less_than_hs + community_data$hs_some_college #collaspe into no college category

community_data <- community_data|> #get rid of dupes and rename
  dplyr::select(-less_than_hs, -hs_some_college)|>
  rename(CD = bach_and_higher )

community_data$college_edu <- ifelse(community_data$CD > 0.5, 1, 0) #make categorical

# 0 = more than 50% of the neighborhood population has less than a college degree
# 1 = more than 50% of the neighborhood population has a college degree

```

#sample proportionately might need to sample at all
```{r}
# proportions <- table(community_data$census_region) / nrow(community_data) # Calculate proportions
# 
# downsize_proportions <- round(proportions * 10000) # Target 10000 total observations
# 
# downsample_groups <- function(community_data, target_counts) {
#   sampled_data <- data.frame()  # Create an empty data frame to store the sampled data
#   
#   for (i in 1:length(target_counts)) {  # Loop through each group's target count
#     group_subset <- subset(community_data, census_region == names(target_counts)[i])  # Subset the data for a specific group
#     sampled_indices <- sample(1:nrow(group_subset), target_counts[i])  # Randomly sample indices based on the target count
#     sampled_group <- group_subset[sampled_indices, ]  # Store the sampled group data
#     
#     sampled_data <- rbind(sampled_data, sampled_group)  # Append sampled rows to the sampled_data data frame
#   }
#   
#   return(sampled_data)  # Return the final sampled dataset
# }

# sampled_community_data <- downsample_groups(community_data, downsize_proportions)
```

### EDA Begins ###

#proportions of no college to college degree by state
```{r}
#collaspe all neighborhood 0 and 1's to represent the education level for the overall state
state_proportion <- community_data |>
  group_by(state, college_edu) |>
  summarise(count = n()) |>
  group_by(state) |>
  mutate(total_state = sum(count)) |>
  mutate(proportion = count / total_state)

#proportion of college education to no college education in each state

```


```{r}
summary <- summary(community_data)

#table that is easier to read
kable(summary, caption = "Summary of Data")

edu_count <- community_data |>
  group_by(college_edu)|>
  count()

```

#correlation matrix
```{r}
subset_data <- community_data[, 6:19]
  
cor_matrix <- cor(subset_data)

ggplot(data = reshape2::melt(cor_matrix), aes(Var2, Var1, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "black") +  
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "College Correlation Heatmap", x = "Variables", y = "Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  

#there are several variables that are correlated with each other above .70. I will compare them to see which one has a higher correlation with the response variable then decide which one to remove
```


#dataset w/ new variables of interest
```{r}
#removed social orgs, casinos, museums, and spec sports 
new_com_data <- community_data|>
  dplyr::select(-7, -8, -11, -17)

subset_data_2 <- new_com_data[, 6:15]
  
cor_matrix_2 <- cor(subset_data_2)

ggplot(data = reshape2::melt(cor_matrix_2), aes(Var2, Var1, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "black") +  
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "College Correlation Heatmap", x = "Variables", y = "Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  

```
#predictor variables are no longer correlated with each other. Remove highly correlated predictors to minimize overfitting. The presence of highly correlated predictors might lead to an unstable model solution.

#number of tobacco stores in the US
```{r}
tobacco_sum_by_state <- new_com_data |>
  group_by(state) |>
  summarise(TotalTobaccoStores = sum(tobacco))

plot_usmap(
   data = tobacco_sum_by_state, values = "TotalTobaccoStores") + 
   scale_fill_continuous(
     low = "white", high = "red", name = "total per 1000", label = scales::comma
   ) + 
   labs(title = "Tobacco stores") +
   theme(legend.position = "right")
```
```{r}
perform_art_sum_by_state <- new_com_data |>
  group_by(state) |>
  summarise(total_perform_art = sum(performing_arts))
  
plot_usmap(
   data = perform_art_sum_by_state, values = "total_perform_art") + 
   scale_fill_continuous(
     low = "white", high = "purple", name = "total per 1000", label = scales::comma
   ) + 
   labs(title = "Performing arts") +
   theme(legend.position = "right")

```
#Libraries
```{r}
lib_sum_by_state <- new_com_data |>
  group_by(state) |>
  summarise(total_lib = sum(libraries))
  

plot_usmap(
   data = lib_sum_by_state, values = "total_lib") + 
   scale_fill_continuous(
     low = "white", high = "purple", name = "total per 1000", label = scales::comma
   ) + 
   labs(title = "Libraries") +
   theme(legend.position = "right")
```
#amusement parks
```{r}
amuse_sum_by_state <- new_com_data |>
  group_by(state) |>
  summarise(total_amuse = sum(amusement_parks))


plot_usmap(
   data = amuse_sum_by_state, values = "total_amuse") + 
   scale_fill_continuous(
     low = "white", high = "purple", name = "total per 1000", label = scales::comma
   ) + 
   labs(title = "Amusement parks") +
   theme(legend.position = "right")

```
#recreation
```{r}
rec_sum_by_state <- new_com_data |>
  group_by(state) |>
  summarise(total_rec = sum(recreation))
 

plot_usmap(
   data = rec_sum_by_state, values = "total_rec") + 
   scale_fill_continuous(
     low = "white", high = "purple", name = "total per 1000", label = scales::comma
   ) + 
   labs(title = "Recreation") +
   theme(legend.position = "right")

```

#fitness
```{r}
fit_sum_by_state <- new_com_data |>
  group_by(state) |>
  summarise(total_fit = sum(fitness))
 

plot_usmap(
   data = fit_sum_by_state, values = "total_fit" ) + 
   scale_fill_continuous(
     low = "white", high = "purple", name = "total per 1000", label = scales::comma
   ) + 
   labs(title = "Fitness centers") +
   theme(legend.position = "right")
```

#liquor
```{r}
liq_sum_by_state <- new_com_data |>
  group_by(state) |>
  summarise(total_liq = sum(liquor_store))

plot_usmap(
   data = liq_sum_by_state, values = "total_liq") + 
   scale_fill_continuous(
     low = "white", high = "purple", name = "total per 1000", label = scales::comma
   ) + 
   labs(title = "Liquor stores") +
   theme(legend.position = "right")

```
#religious orgs
```{r}
relig_sum_by_state <- new_com_data |>
  group_by(state) |>
  summarise(total_relig = sum(religious_orgs))

plot_usmap(
   data = relig_sum_by_state, values = "total_relig") + 
   scale_fill_continuous(
     low = "white", high = "purple", name = "total per 1000", label = scales::comma
   ) + 
   labs(title = "Religious Orgs") +
   theme(legend.position = "right")

```
#does not account for the different number of people in each state
#dont know if data from this website is even accurate
#education information is the mean of the years 2013-2017 so thats probably not that accurate either
```{r}
#map of proportion of residents that have a college degree across the US
education_state <- state_proportion|>
  group_by(state)|>
  filter(college_edu == "1")

plot_usmap(
   data = education_state, values = "proportion") + 
   scale_fill_continuous(
     low = "white", high = "red", name = "proportion", label = scales::comma
   ) + 
   labs(title = "College Education") +
   theme(legend.position = "right")
```

```{r}
no_education_state <- state_proportion|>
  group_by(state)|>
  filter(college_edu == "0")

plot_usmap(
   data = no_education_state, values = "proportion", lines = "blue"
 ) + 
   scale_fill_continuous(
     low = "white", high = "purple", name = "proportion", label = scales::comma
   ) + 
   labs(title = "No College Education") +
   theme(legend.position = "right")


```

###Model building###

```{r}
#checking class distribution
table(new_com_data$college_edu)

ggplot(new_com_data, aes(x = factor(college_edu))) +
  geom_bar() +
  labs(title = "Class Distribution of college_edu")
```
#try to fix class imbalance by undersampling and oversampling
```{r}
  balanced_com_data <- ovun.sample(college_edu ~ performing_arts + 
                              libraries + 
                              amusement_parks + 
                              recreation + 
                              fitness + 
                              liquor_store + 
                              tobacco +
                              religious_orgs +
                              state, 
                              data = new_com_data,
                              method = "both", 
                              N = 72410,
                              seed = 678)$data

  table(balanced_com_data$college_edu)
```

#split into test and training
```{r}
 set.seed(678)
 
 training_samples <- balanced_com_data$college_edu |>
   createDataPartition(p = 0.8, list = FALSE)
 
 train_data  <- balanced_com_data[training_samples, ]
 test_data <- balanced_com_data[-training_samples, ]

```

#Null Model
```{r}

null <- glm(college_edu ~ 1, train_data, family = binomial)

summary(null)

predicted_probs_1 <- predict(null, newdata = test_data, type = "response")
predicted_classes_1 <- ifelse(predicted_probs_1 > 0.5, 1, 0)
actual_classes_1 <- test_data$college_edu
 
# Calculate misclassification error
misclassification_error_1 <- mean(predicted_classes_1 != actual_classes_1)
 
print(paste("Misclassification Error:", misclassification_error_1))

```

#got rid of a few predictors because for some reason they were "perfect predictors"?
```{r}
complete_pooling <- glm(college_edu ~ performing_arts + 
                              libraries + 
                              amusement_parks + 
                              recreation + 
                              fitness + 
                              liquor_store + 
                              tobacco +
                              religious_orgs,
                 data = train_data, 
                 family = binomial(link="logit"))

summary(complete_pooling, corr = FALSE)

predicted_probs_2 <- predict(complete_pooling, newdata = test_data, type = "response")
predicted_classes_2 <- ifelse(predicted_probs_2 > 0.5, 1, 0)
actual_classes_2 <- test_data$college_edu
 
# Calculate misclassification error
misclassification_error_2 <- mean(predicted_classes_2 != actual_classes_2)
 
print(paste("Misclassification Error:", misclassification_error_2))

```

#no pooling
```{r}
no_pooling <- glm(college_edu ~ libraries + amusement_parks + liquor_store + tobacco + factor(state),
                          data = train_data, 
                          family = binomial(link = "logit"))

summary(no_pooling, corr = FALSE)

predicted_probs_3 <- predict(no_pooling, newdata = test_data, type = "response")
predicted_classes_3 <- ifelse(predicted_probs_3 > 0.5, 1, 0)
actual_classes_3 <- test_data$college_edu
 
# Calculate misclassification error
misclassification_error_3 <- mean(predicted_classes_3 != actual_classes_3)
 
print(paste("Misclassification Error:", misclassification_error_3))

```


#partial pooling
```{r}
#varying intercept
partial_pooling <- glmer(college_edu ~ libraries + amusement_parks + liquor_store + tobacco + (1 | state), 
                          data = train_data, 
                          family = binomial(link = "logit"))

summary(partial_pooling, corr = FALSE)

predicted_probs_4 <- predict(partial_pooling, newdata = test_data, type = "response")
predicted_classes_4 <- ifelse(predicted_probs_4 > 0.5, 1, 0)
actual_classes_4 <- test_data$college_edu
 
# Calculate misclassification error
misclassification_error_4 <- mean(predicted_classes_4 != actual_classes_4)
 
print(paste("Misclassification Error:", misclassification_error_4))
```

#partial pooling
```{r}
#varying intercept varying slope
partial_pooling_2 <- glmer(college_edu ~ libraries + amusement_parks + liquor_store + tobacco + 
                          (1 + libraries | state) + (1 + amusement_parks | state) + (1 + liquor_store | state) + 
                          (1 + tobacco | state), 
                               data = train_data, 
                               family = binomial(link = "logit"))
                         

summary(partial_pooling_2, corr = FALSE)

predicted_probs_5 <- predict(partial_pooling_2, newdata = test_data, type = "response")
predicted_classes_5 <- ifelse(predicted_probs_5 > 0.5, 1, 0)
actual_classes_5 <- test_data$college_edu
 
# Calculate misclassification error
misclassification_error_5 <- mean(predicted_classes_5 != actual_classes_5)
 
print(paste("Misclassification Error:", misclassification_error_5))
```

#partial pooling varying slope
```{r}
partial_pooling_3 <- glmer(college_edu ~ libraries + amusement_parks + liquor_store + tobacco + 
                          (libraries | state) + (amusement_parks | state) + (liquor_store | state) + 
                          (tobacco | state), 
                               data = train_data, 
                               family = binomial(link = "logit"))

summary(partial_pooling_3, corr = FALSE)

predicted_probs_6 <- predict(partial_pooling_3, newdata = test_data, type = "response")
predicted_classes_6 <- ifelse(predicted_probs_6 > 0.5, 1, 0)
actual_classes_6 <- test_data$college_edu
 
# Calculate misclassification error
misclassification_error_6 <- mean(predicted_classes_6 != actual_classes_6)
 
print(paste("Misclassification Error:", misclassification_error_6))

```

#table for AIC values. Model comparison
```{r}

aic_values <- c(AIC(null), AIC(complete_pooling), AIC(no_pooling), AIC(partial_pooling), AIC(partial_pooling_2), AIC(partial_pooling_3))

model_names <- c("Null model", "Complete pooling model", "No pooling model", "Partial pooling model vary intercept", "Partial pooling model vary both", "Partial pooling model vary slope")

table_data <- data.frame(Model = model_names, AIC = aic_values)

aic_ktable <- kable(table_data, caption = "AIC values of Models", align = c("l", "c"))
  
print(aic_ktable)


```


#table for Misclassification error. Model validation.
```{r}
misclassification_errors <- c(0.50, 0.30, 0.40, 0.40, 0.42, 0.41)

model_names <- c("Null model", "Complete pooling model", "No pooling model", "Partial pooling model vary intercept", "Partial pooling model vary both", "Partial pooling model vary slope")

misclassification_table <- data.frame(Model = model_names, 'Misclassification Error' = misclassification_errors)

missclass_ktable <- kable(misclassification_table, caption = "Misclassification Errors of Models", align = c("l", "c"))

print(missclass_ktable)


```